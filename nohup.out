/disk/data4/zbrunner/WhisperBiasing/whisper/__init__.py:115: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(fp, map_location=device)
/disk/data4/zbrunner/WhisperBiasing/dataloader.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  fbank = torch.load(data_path)
Start of training
200 / 28539 steps finished in 99.64090013504028 | Loss: 0.11677762961015105 | lr: 0.0005
400 / 28539 steps finished in 205.30173420906067 | Loss: 0.10510472923517228 | lr: 0.0005
600 / 28539 steps finished in 321.0686857700348 | Loss: 0.10690409878268838 | lr: 0.0005
800 / 28539 steps finished in 436.3069577217102 | Loss: 0.09826108396053314 | lr: 0.0005
1000 / 28539 steps finished in 551.8776786327362 | Loss: 0.10310879280790687 | lr: 0.0005
1200 / 28539 steps finished in 668.1400275230408 | Loss: 0.08762647320516408 | lr: 0.0005
1400 / 28539 steps finished in 784.6861276626587 | Loss: 0.07775558365508914 | lr: 0.0005
1600 / 28539 steps finished in 902.2977361679077 | Loss: 0.07257412790786474 | lr: 0.0005
1800 / 28539 steps finished in 1018.1670105457306 | Loss: 0.07385329204611481 | lr: 0.0005
2000 / 28539 steps finished in 1133.9499316215515 | Loss: 0.07628823839128017 | lr: 0.0005
2200 / 28539 steps finished in 1249.50994515419 | Loss: 0.07521800760179759 | lr: 0.0005
2400 / 28539 steps finished in 1365.2116932868958 | Loss: 0.07270213960669934 | lr: 0.0005
2600 / 28539 steps finished in 1480.5337083339691 | Loss: 0.07547090359032155 | lr: 0.0005
2800 / 28539 steps finished in 1596.630733013153 | Loss: 0.07081934446468949 | lr: 0.0005
3000 / 28539 steps finished in 1713.241196155548 | Loss: 0.07137500622309745 | lr: 0.0005
3200 / 28539 steps finished in 1829.115933895111 | Loss: 0.06876066939905286 | lr: 0.0005
3400 / 28539 steps finished in 1946.2355699539185 | Loss: 0.06899347663857043 | lr: 0.0005
3600 / 28539 steps finished in 2062.1872115135193 | Loss: 0.07361022868193685 | lr: 0.0005
3800 / 28539 steps finished in 2177.936816930771 | Loss: 0.06871086486615241 | lr: 0.0005
4000 / 28539 steps finished in 2294.185669183731 | Loss: 0.07183272465132177 | lr: 0.0005
4200 / 28539 steps finished in 2411.2202315330505 | Loss: 0.07533278492279351 | lr: 0.0005
4400 / 28539 steps finished in 2527.3472225666046 | Loss: 0.07279220199212431 | lr: 0.0005
4600 / 28539 steps finished in 2642.1730077266693 | Loss: 0.07471278963144869 | lr: 0.0005
4800 / 28539 steps finished in 2757.4031550884247 | Loss: 0.0743572951387614 | lr: 0.0005
5000 / 28539 steps finished in 2872.7619903087616 | Loss: 0.07512084110174327 | lr: 0.0005
5200 / 28539 steps finished in 2987.483820438385 | Loss: 0.07474689457565546 | lr: 0.0005
5400 / 28539 steps finished in 3097.890466928482 | Loss: 0.07209335987456143 | lr: 0.0005
5600 / 28539 steps finished in 3208.3100645542145 | Loss: 0.07093685621861369 | lr: 0.0005
5800 / 28539 steps finished in 3321.7142040729523 | Loss: 0.07174184607341885 | lr: 0.0005
6000 / 28539 steps finished in 3438.682473897934 | Loss: 0.0708136144420132 | lr: 0.0005
6200 / 28539 steps finished in 3552.8117847442627 | Loss: 0.07276378475129604 | lr: 0.0005
6400 / 28539 steps finished in 3667.212355852127 | Loss: 0.07315358699299396 | lr: 0.0005
6600 / 28539 steps finished in 3782.068712949753 | Loss: 0.06850944588892162 | lr: 0.0005
6800 / 28539 steps finished in 3898.8872764110565 | Loss: 0.07617605349048973 | lr: 0.0005
7000 / 28539 steps finished in 4015.7488985061646 | Loss: 0.07219742885790766 | lr: 0.0005
7200 / 28539 steps finished in 4130.257077217102 | Loss: 0.07326358643360437 | lr: 0.0005
7400 / 28539 steps finished in 4247.622555971146 | Loss: 0.06661091825924814 | lr: 0.0005
7600 / 28539 steps finished in 4364.184871673584 | Loss: 0.07199963414110243 | lr: 0.0005
7800 / 28539 steps finished in 4475.0425572395325 | Loss: 0.06957045434042812 | lr: 0.0005
8000 / 28539 steps finished in 4584.220815420151 | Loss: 0.0732673451770097 | lr: 0.0005
8200 / 28539 steps finished in 4694.179194688797 | Loss: 0.07198106365278363 | lr: 0.0005
8400 / 28539 steps finished in 4806.321915388107 | Loss: 0.07432012165896594 | lr: 0.0005
8600 / 28539 steps finished in 4917.231390237808 | Loss: 0.07340384695678949 | lr: 0.0005
8800 / 28539 steps finished in 5027.561156749725 | Loss: 0.07274078398011624 | lr: 0.0005
9000 / 28539 steps finished in 5137.73708486557 | Loss: 0.0767393880803138 | lr: 0.0005
9200 / 28539 steps finished in 5249.233311653137 | Loss: 0.07075013489462435 | lr: 0.0005
9400 / 28539 steps finished in 5360.7678298950195 | Loss: 0.07031216347590089 | lr: 0.0005
9600 / 28539 steps finished in 5473.737291097641 | Loss: 0.07201255382038653 | lr: 0.0005
9800 / 28539 steps finished in 5585.81512761116 | Loss: 0.07271919756196439 | lr: 0.0005
10000 / 28539 steps finished in 5697.451168298721 | Loss: 0.07462304739281535 | lr: 0.0005
10200 / 28539 steps finished in 5810.730619668961 | Loss: 0.06726661909371615 | lr: 0.0005
10400 / 28539 steps finished in 5924.756739139557 | Loss: 0.07009465585462749 | lr: 0.0005
10600 / 28539 steps finished in 6037.078978538513 | Loss: 0.07014763694256544 | lr: 0.0005
10800 / 28539 steps finished in 6149.523835659027 | Loss: 0.0691605866048485 | lr: 0.0005
11000 / 28539 steps finished in 6261.893396139145 | Loss: 0.07596324121579529 | lr: 0.0005
11200 / 28539 steps finished in 6376.012150526047 | Loss: 0.0737507182918489 | lr: 0.0005
11400 / 28539 steps finished in 6490.775771856308 | Loss: 0.07305368865840137 | lr: 0.0005
11600 / 28539 steps finished in 6604.896859884262 | Loss: 0.07220909767784178 | lr: 0.0005
11800 / 28539 steps finished in 6719.110931158066 | Loss: 0.07173795666545629 | lr: 0.0005
12000 / 28539 steps finished in 6833.603626012802 | Loss: 0.06987365689128637 | lr: 0.0005
12200 / 28539 steps finished in 6946.528615951538 | Loss: 0.06877033055759967 | lr: 0.0005
12400 / 28539 steps finished in 7060.563212633133 | Loss: 0.07689168371260166 | lr: 0.0005
12600 / 28539 steps finished in 7175.762964010239 | Loss: 0.07093425115570426 | lr: 0.0005
12800 / 28539 steps finished in 7290.806849002838 | Loss: 0.07378069019876421 | lr: 0.0005
13000 / 28539 steps finished in 7405.858456850052 | Loss: 0.07128729593008756 | lr: 0.0005
13200 / 28539 steps finished in 7521.216870307922 | Loss: 0.0700063663162291 | lr: 0.0005
13400 / 28539 steps finished in 7636.656058311462 | Loss: 0.07034320695325733 | lr: 0.0005
13600 / 28539 steps finished in 7751.771347045898 | Loss: 0.07003535609692335 | lr: 0.0005
13800 / 28539 steps finished in 7865.996867179871 | Loss: 0.07273304857313632 | lr: 0.0005
14000 / 28539 steps finished in 7980.162305593491 | Loss: 0.07012138597667217 | lr: 0.0005
14200 / 28539 steps finished in 8095.353815793991 | Loss: 0.06959389942698181 | lr: 0.0005
14400 / 28539 steps finished in 8210.118092298508 | Loss: 0.07032630021683872 | lr: 0.0005
14600 / 28539 steps finished in 8322.258818626404 | Loss: 0.06737972908653318 | lr: 0.0005
14800 / 28539 steps finished in 8435.020646095276 | Loss: 0.07190742534585297 | lr: 0.0005
15000 / 28539 steps finished in 8548.267548322678 | Loss: 0.07328990908339619 | lr: 0.0005
15200 / 28539 steps finished in 8662.790766239166 | Loss: 0.07048330251127481 | lr: 0.0005
15400 / 28539 steps finished in 8776.251119613647 | Loss: 0.07291557481512427 | lr: 0.0005
15600 / 28539 steps finished in 8890.204118013382 | Loss: 0.07321206749416888 | lr: 0.0005
15800 / 28539 steps finished in 9002.850323200226 | Loss: 0.07192609664984047 | lr: 0.0005
16000 / 28539 steps finished in 9115.91808962822 | Loss: 0.06968250109814107 | lr: 0.0005
16200 / 28539 steps finished in 9228.523882865906 | Loss: 0.06986547500826418 | lr: 0.0005
16400 / 28539 steps finished in 9341.026386499405 | Loss: 0.069816117817536 | lr: 0.0005
16600 / 28539 steps finished in 9452.68348789215 | Loss: 0.07334677794016897 | lr: 0.0005
16800 / 28539 steps finished in 9562.781316518784 | Loss: 0.06918818171136082 | lr: 0.0005
17000 / 28539 steps finished in 9673.793827533722 | Loss: 0.07372686366550624 | lr: 0.0005
17200 / 28539 steps finished in 9785.115763425827 | Loss: 0.0695125752221793 | lr: 0.0005
17400 / 28539 steps finished in 9894.963499307632 | Loss: 0.07104242239147425 | lr: 0.0005
17600 / 28539 steps finished in 10005.307571411133 | Loss: 0.07386673054657876 | lr: 0.0005
17800 / 28539 steps finished in 10116.011645078659 | Loss: 0.07047031266614795 | lr: 0.0005
18000 / 28539 steps finished in 10228.382923603058 | Loss: 0.07022372662089765 | lr: 0.0005
18200 / 28539 steps finished in 10339.200357675552 | Loss: 0.07354547567665577 | lr: 0.0005
18400 / 28539 steps finished in 10450.274639368057 | Loss: 0.07195765094831585 | lr: 0.0005
18600 / 28539 steps finished in 10562.085947751999 | Loss: 0.07003629649057984 | lr: 0.0005
18800 / 28539 steps finished in 10673.578774690628 | Loss: 0.07429544298909604 | lr: 0.0005
19000 / 28539 steps finished in 10784.736698389053 | Loss: 0.07263433679938316 | lr: 0.0005
19200 / 28539 steps finished in 10895.502679347992 | Loss: 0.06763051161542535 | lr: 0.0005
19400 / 28539 steps finished in 11006.29958486557 | Loss: 0.0665496436227113 | lr: 0.0005
19600 / 28539 steps finished in 11115.641150712967 | Loss: 0.07184776305221022 | lr: 0.0005
19800 / 28539 steps finished in 11226.421462535858 | Loss: 0.06753554652445018 | lr: 0.0005
20000 / 28539 steps finished in 11336.652916908264 | Loss: 0.07038286394905299 | lr: 0.0005
20200 / 28539 steps finished in 11447.86405491829 | Loss: 0.07022451477125287 | lr: 0.0005
20400 / 28539 steps finished in 11558.589540243149 | Loss: 0.06986663538962602 | lr: 0.0005
20600 / 28539 steps finished in 11669.74723315239 | Loss: 0.06973558271303773 | lr: 0.0005
20800 / 28539 steps finished in 11780.046051502228 | Loss: 0.067155147427693 | lr: 0.0005
21000 / 28539 steps finished in 11892.030655622482 | Loss: 0.06967444780282676 | lr: 0.0005
21200 / 28539 steps finished in 12000.972846746445 | Loss: 0.07307420936413109 | lr: 0.0005
21400 / 28539 steps finished in 12112.055074214935 | Loss: 0.06621342443861067 | lr: 0.0005
21600 / 28539 steps finished in 12222.571747303009 | Loss: 0.07431144881993532 | lr: 0.0005
21800 / 28539 steps finished in 12332.804720878601 | Loss: 0.07556834363378584 | lr: 0.0005
22000 / 28539 steps finished in 12443.22489619255 | Loss: 0.07044781121425331 | lr: 0.0005
22200 / 28539 steps finished in 12554.008688211441 | Loss: 0.07163451628759504 | lr: 0.0005
22400 / 28539 steps finished in 12665.055460691452 | Loss: 0.07022790339309722 | lr: 0.0005
22600 / 28539 steps finished in 12776.299351930618 | Loss: 0.06989124340005219 | lr: 0.0005
22800 / 28539 steps finished in 12887.43298959732 | Loss: 0.07339193844236433 | lr: 0.0005
23000 / 28539 steps finished in 12998.452049970627 | Loss: 0.06918846880085766 | lr: 0.0005
23200 / 28539 steps finished in 13108.623176574707 | Loss: 0.06710001754574478 | lr: 0.0005
23400 / 28539 steps finished in 13220.412904977798 | Loss: 0.07011164612136782 | lr: 0.0005
23600 / 28539 steps finished in 13330.32442164421 | Loss: 0.06923082882538438 | lr: 0.0005
23800 / 28539 steps finished in 13441.436321735382 | Loss: 0.06707672278396785 | lr: 0.0005
24000 / 28539 steps finished in 13552.007411003113 | Loss: 0.07370295439846813 | lr: 0.0005
24200 / 28539 steps finished in 13662.67545223236 | Loss: 0.06928440389223396 | lr: 0.0005
24400 / 28539 steps finished in 13773.380399465561 | Loss: 0.07234676175750793 | lr: 0.0005
24600 / 28539 steps finished in 13883.567589759827 | Loss: 0.07335840131156146 | lr: 0.0005
24800 / 28539 steps finished in 13993.638447999954 | Loss: 0.07039585483726114 | lr: 0.0005
25000 / 28539 steps finished in 14103.414054870605 | Loss: 0.06858628795482219 | lr: 0.0005
25200 / 28539 steps finished in 14213.835284948349 | Loss: 0.06641597820445895 | lr: 0.0005
25400 / 28539 steps finished in 14324.142976522446 | Loss: 0.06810233565978706 | lr: 0.0005
25600 / 28539 steps finished in 14433.484717130661 | Loss: 0.06783801874145866 | lr: 0.0005
25800 / 28539 steps finished in 14543.151532888412 | Loss: 0.06499011586420238 | lr: 0.0005
26000 / 28539 steps finished in 14652.832411766052 | Loss: 0.06768108675256372 | lr: 0.0005
26200 / 28539 steps finished in 14763.366712331772 | Loss: 0.06859453202225269 | lr: 0.0005
26400 / 28539 steps finished in 14873.618043661118 | Loss: 0.06729562015272678 | lr: 0.0005
26600 / 28539 steps finished in 14985.67023563385 | Loss: 0.06944591137580573 | lr: 0.0005
26800 / 28539 steps finished in 15096.203552484512 | Loss: 0.06797120382077992 | lr: 0.0005
27000 / 28539 steps finished in 15206.825141191483 | Loss: 0.06921879937872291 | lr: 0.0005
27200 / 28539 steps finished in 15318.210318803787 | Loss: 0.06866392895113677 | lr: 0.0005
27400 / 28539 steps finished in 15428.654750585556 | Loss: 0.06984860377851873 | lr: 0.0005
27600 / 28539 steps finished in 15540.244599819183 | Loss: 0.06709268793463707 | lr: 0.0005
27800 / 28539 steps finished in 15651.074759960175 | Loss: 0.06494635000824928 | lr: 0.0005
28000 / 28539 steps finished in 15761.214025735855 | Loss: 0.0693318354897201 | lr: 0.0005
28200 / 28539 steps finished in 15870.618249177933 | Loss: 0.06962299191392958 | lr: 0.0005
28400 / 28539 steps finished in 15981.322335481644 | Loss: 0.06771005478687585 | lr: 0.0005
Traceback (most recent call last):
  File "/disk/data4/zbrunner/WhisperBiasing/train.py", line 174, in <module>
    for idx, data in enumerate(devloader):
  File "/disk/data4/zbrunner/whisper_biasing/espnet/tools/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 701, in __next__
    data = self._next_data()
  File "/disk/data4/zbrunner/whisper_biasing/espnet/tools/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 757, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/disk/data4/zbrunner/whisper_biasing/espnet/tools/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/disk/data4/zbrunner/whisper_biasing/espnet/tools/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/disk/data4/zbrunner/WhisperBiasing/dataloader.py", line 32, in __getitem__
    fbank = torch.load(data_path)
  File "/disk/data4/zbrunner/whisper_biasing/espnet/tools/venv/lib/python3.10/site-packages/torch/serialization.py", line 1319, in load
    with _open_file_like(f, "rb") as opened_file:
  File "/disk/data4/zbrunner/whisper_biasing/espnet/tools/venv/lib/python3.10/site-packages/torch/serialization.py", line 659, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/disk/data4/zbrunner/whisper_biasing/espnet/tools/venv/lib/python3.10/site-packages/torch/serialization.py", line 640, in __init__
    super().__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: 'data/LibriSpeech/dev-other/700/122867/700-122867-0021_fbank.pt'
